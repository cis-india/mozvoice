<!DOCTYPE html>
<html><!DOCTYPE html>
<html>
<head>
  <!-- Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
  <meta name="description" content="Making Voices Heard | A study by the Centre for Internet and Society, India, supported by Mozilla Corporation" />
  <!-- Title + CSS + Favicon -->
  <title>Making Voices Heard</title>
  <link rel="stylesheet" type="text/css" href="css/semantic.min.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="shortcut icon" type="image/x-icon" href="img/favicon.ico" />
  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/4c415b9185.js" crossorigin="anonymous"></script>
</head>
<body>
  <!-- Header -->
  <div>
    <div class="ui fluid container banner">
      <div class="banner-image"></div>
    </div>
  </div>
  <!-- Top Navigation Bar -->
  <div class="blue nav">
    <div class="ui container">
      <div class="nav-entries">
        <a href="index.html" id="home">Home</a> <a href="design-brief.html">Design Brief</a> <span id="inactive">Policy Brief</span> <a href="mapping-actors.html">Mapping Actors</a> <a href="#case-studies">Case Studies</a> <a href="#literature-surveys">Literature Surveys</a> <a href="#resources">Resources</a> <span id="report"><a href="docs/CIS_MakingVoicesHeard_Report.pdf"><i class="fas fa-arrow-circle-down"></i> Get Full Report</a></span>
      </div>
    </div>
  </div>
  <!-- Title -->
  <div class="grey">
    <div class="ui container four column stackable grid">
      <div class="one wide column empty">
      </div>
      <div class="fourteen wide column text">
        <h2>Evolution and Typology of Voice Interfaces</h2>
      </div>
      <div class="one wide column empty">
      </div>
      <div class="one wide column empty">
      </div>
      <div class="nine wide column text">

        <h3 id="about">Background</h3>

<p> The availability of multiple modes of interaction such as voice and gesture makes devices accessible to a wide variety of people. Voice interfaces (VI), in particular, create a level playing field for those who are limited by single-language, text-based interfaces.</p>
<p> Schnelle-Walka defines VIs as “user interfaces using speech input through a speech recognizer and speech output through speech synthesis or prerecorded audio”.<sup class="superscript"><a href="#fn1">1</a></sup><a name="ref1"></a> In essence, VI technologies involve two processes: one converting the language to code that a computer understands, and converting the computer language back to a language that the human understands. Considering that the predominant means of input for VIs is speech, they are also known as natural language interfaces.<sup class="superscript"><a href="#fn2">2</a></sup><a name="ref2"></a></p>
<h3 id="about">Tracing the evolution of VIs</h3>
<p> Before Siri and Alexa, we had ‘Audrey’, created by Bell Laboratories’ Harry Fletcher and Homer Dudley, who are considered the pioneers of VIs for their groundbreaking research on speech synthesis and human speech modelling.<sup class="superscript"><a href="#fn3">3</a></sup><a name="ref3"></a> In 1952, Audrey was used for number recognition through spoken input.<sup class="superscript"><a href="#fn4">4</a></sup><a name="ref4"></a> A decade later, IBM’s ‘Shoebox’ could not only recognise digits from zero to nine but also comprehend 16 words.<sup class="superscript"><a href="#fn5">5</a></sup><a name="ref5"></a></p>
<p> In 1992, AT&T Telefonica developed a speech-to-speech prototype, VESTS (Voice English/Spanish Translator), which relied heavily on spoken language translation.<sup class="superscript"><a href="#fn6">6</a></sup><a name="ref6"></a> VESTS, a speaker-trained system that could process over 450 words, was exhibited at the Seville World's Fair in Spain. VIs have come a long way from these early prototypes to modern voice assistants, such as Alexa, Siri, Cortana, and the Google Assistant, which are now accessible to consumers worldover.<sup class="superscript"><a href="#fn7">7</a></sup><a name="ref7"></a></p>
<p> One of the main reasons for the proliferation of VIs today is that since 2012 smartphones come with a built-in VI. According to a 2018 PwC survey, consumers issued voice commands most commonly on smartphones from among a plethora of voice-enabled devices.<sup class="superscript"><a href="#fn8">8</a></sup><a name="ref8"></a> Mobile phones now operate almost like ‘shrunken desktops’ because of their inherent operational versatility. However, the reduced screen size is the primary structural limitation of these devices. To overcome this limitation, voice has become an important input to complete tasks without having to use the touch function or type on their phones.<sup class="superscript"><a href="#fn9">9</a></sup><a name="ref9"></a> Hence, developers have now integrated cloud-based voice technologies into devices – as in the case of Amazon Echo and Google Home as well as through open-source initiatives such as Mozilla’s Deep Speech, which is an open-source speech-to-text engine.<sup class="superscript"><a href="#fn10">10</a></sup><a name="ref10"></a></p>
<h3 id="about">Features of VIs</h3>
<p>In the early 90s, researchers identified the five basic elements<sup class="superscript"><a href="#fn11">11</a></sup><a name="ref11"></a> of voice processing technologies:</p>

        <ol>
           <li><p><b>Voice coding:</b> the process of compressing the information transmitted through the voice signal to transmit or store it economically in systems of a lower capacity. </li></p>
<p> </p>
           <li><p><b>Voice synthesis:</b> the synthetic replication of voice signals to facilitate the transmission of information from machine to human.</li></p>
<p> </p>
           <li><p><b>Speech recognition:</b> the extraction of information that is there in a voice signal to control the actions taken by the device in response to spoken commands.<sup class="superscript"><a href="#fn12">12</a></sup><a name="ref12"></a></li></p>
           <li><p><b>Spoken language translation:</b>  On recognising the language the person is speaking in, the translation of a message from one language to another. Through this process, two individuals who do not speak the same language can communicate.<sup class="superscript"><a href="#fn13">13</a></sup><a name="ref13"></a></li></p>
          <p> Voice output is of two distinct categories: pre-recorded speech and synthetic speech.<sup class="superscript"><a href="#fn14">14</a></sup><a name="ref14"></a> Pre-recorded speech is natural speech that is recorded and stored for future use. In contrast, synthetic speech employs natural language processing (NLP) for the automatic generation of appropriate natural-language responses or output in the form of written text.<sup class="superscript"><a href="#fn15">15</a></sup><a name="ref15"></a></p>
<p> </p>
</ol>
        </p>
<p>NLP involves the conversion of textual information into speech and vice-versa, which enables a device to discern and process natural language data. The system then processes this data by standardising text inputs and splitting it into words and sentences. Then, the device can ascertain the syntax of the input provided. NLP comprises two main natural-language principles:<sup class="superscript"><a href="#fn16">16</a></sup><a name="ref16"></a> </p>

<ol>
   <li><p><b>Natural language understanding (NLU):</b> NLU is a branch of NLP that deals with reading comprehension, synonyms, themes, and lexical semantics. It is used to construct the responses of VIs through algorithms.<sup class="superscript"><a href="#fn17">17</a></sup><a name="ref17"></a></li></p>

<p> </p>
   <li><p><b>Natural language generation (NLG):</b> The first step of NLG involves processing relevant content from databases. This is followed by sentence planning, which involves the formation of natural-language responses through text realisation. As a consequence, the NLG process delivers a meaningful and personalised response, as opposed to a pre-scripted one .<sup class="superscript"><a href="#fn18">18</a></sup><a name="ref18"></a></li></p>
<p> </p>
</ol>
</p>
<p>Synthetic speech employs NLP for its characteristically high ‘segmental intelligibility’ – or its ability to understand each segment of speech. However, pre-recorded speech outputs tend to be preferred by all for their human voice and pronunciation characteristics. These characteristics exist on the condition that the pre-recorded speech maintains the delicate balance between natural prosody<sup class="superscript"><a href="#fn19">19</a></sup><a name="ref19"></a> and the recorded elements. Since it successfully maintains the quality of natural speech, the natural prosody of pre-recorded speech output is higher than that of synthetic speech.<sup class="superscript"><a href="#fn20">20</a></sup><a name="ref20"></a></p>

<p> Without this characteristic, the interface would be subject to systemic fluctuations in acoustic signals.<sup class="superscript"><a href="#fn6">6</a></sup><a name="ref6"></a> This would lead to modifications in input conditions which would minimally degrade the performance of the interface.<sup class="superscript"><a href="#fn7">7</a></sup><a name="ref7"></a> Building standardised models, thereby, would enable individuals  to interact with VIs with high accuracy levels.<sup class="superscript"><a href="#fn8">8</a></sup><a name="ref8"></a></p>

<h3 id="significant">Significant challenges for multilingual support</h3>
<p> In an empirical study conducted by Dyches et al ,<sup class="superscript"><a href="#fn9">9</a></sup><a name="ref9"></a> 724 participants in Ohio were approached to assess the current state of the interactive voice response (IVR) system for non acute primary care. However, only 42% of the participants were able to finish the telephone screening. The rest were not able to complete the IVR process in the research for several reasons. One of the most significant reasons cited was not knowing English. Hence, developing a VI in all local, regional languages would be a step towards making digital spaces truly democratic.</p>
<p> This idea, however, has not come to fruition because of the challenge involved in developing VIs in local languages. The major challenge is  further reflected in a W3Tech survey, as depicted in Graph 1, which reveals that English was used by 59.5% of approximately 10 million global websites as of June 2020.<sup class="superscript"><a href="#fn10">10</a></sup><a name="ref10"></a> The websites surveyed by W3Tech, however, include only websites that use technology and have “useful content”. To elaborate further, default web server pages and websites owned by domain spammers were excluded from the survey. In addition, subdomains and redirected domains were not included in the survey.</p>
<p>The aforementioned statistics become even more significant when we consider global demographics – only 527 million people in the world, out of approximately 7.2 billion, are native English speaking people.<sup class="superscript"><a href="#fn11">11</a></sup><a name="ref11"></a> The population of native speakers<sup class="superscript"><a href="#fn12">12</a></sup><a name="ref12"></a> of three languages, namely, all Chinese dialects combined, Hindi, and Urdu is higher than the native English speaking population.<sup class="superscript"><a href="#fn13">13</a></sup><a name="ref13"></a> However, the use of these languages in website content in the two most populous countries namely China and India, are minuscule in terms of percentage. For China, it stands at 1.50%, while Hindi is behind at 0.1%. However, less than 0.1% of the 10 million (approximate value) websites surveyed accounted for using Indic languages such as Bengali, Kannada, Tamil, Telugu, Marathi, Punjabi, Gujarati, Oriya Urdu, and Assamese.<sup class="superscript"><a href="#fn14">14</a></sup><a name="ref14"></a></p>
<p>Ultimately, to address language-related challenges, building an efficient VI equipped with multilingual support is the need of the hour. This requires the expertise of computational linguists to create the domain model –i.e., build the lexicon for NLU systems and fine-tune and debug the grammar for the same.<sup class="superscript"><a href="#fn15">15</a></sup><a name="ref15"></a> Another main challenge is that it remains an expensive procedure as it requires the labour of individuals with a very niche skill set.<sup class="superscript"><a href="#fn16">16</a></sup><a name="ref16"></a> Similarly, Levinson (1994) opines that the language accessibility barriers of VIs are predominantly compounded by the lack of technical expertise to create such devices. Though the recent trend of consumer facing VIs show that there is no dearth of technical expertise, the particular nature of voice and languages still create technological challenges.</p>
<p>To summarise, the reluctance to develop VIs in several languages is primarily linked to the low scope for profitability and the labour-intensive requirement of computational linguists. In addition to these factors, several additional impediments have been identified for the development of interfaces in (non-dominant) local languages:</p>


    <h3 id="voice">Voice initiatives to bridge the digital divide</h3>
<p> Paul (2017) opines that a possible method to resolve the linguistic limitations of VIs, is to train the device employing a VI to associate particular sounds with words.<sup class="superscript"><a href="#fn42">42</a></sup><a name="ref42"></a> Training a machine to recognise sounds requires an extensive database of voice recordings on a wide variety of topics. The flexibility and accuracy of the VI are dependent on the number of voices and accents it is exposed to.<sup class="superscript"><a href="#fn43">43</a></sup><a name="ref43"></a> Presently, several eminent organisations, universities, and government bodies have undertaken the challenging task of creating such extensive voice databases:</p>
<ol>
   <li><p><b>Global initiatives </b> In an attempt to create a database to foster the growth of inclusive technologies, the Mozilla Foundation launched the Common Voice project in 2017.<sup class="superscript"><a href="#fn44">44</a></sup><a name="ref44"></a> To facilitate machine learning vis-a-vis VIs, developers require a large amount of voice data, which is usually expensive and resource-intensive to collect. Hence, Common Voice encourages people to donate their voice recording samples as well as verify other voice clips, thereby creating an accurate, open-source, and truly diverse database of voices.<sup class="superscript"><ahref="#fn45">45</a></sup><a name="ref45"></a> The project also recently initiated work on collecting single word segments, which aims to enable the machine to identify numbers (zero to nine) and the words ‘yes’, ‘no’, ‘hey’, and ‘Firefox’.<sup class="superscript"><a href="#fn46">46</a></sup><a name="ref46"></a>As of  July 2021, the Common Voice project had collected 13,905 hours of recordings in 76 different languages.sup class="superscript"><a href="#fn47">47</a></sup><a name="ref47"></a></li></p>
    <p>The Linguistic Data Consortium (LDC) was conceptualised in 1992 to enhance technologies to support language-based academia.<sup class="superscript"><a href="#fn48">48</a></sup><a name="ref48"></a> LDC served as the leading language repository for educational institutions, corporations, and research institutes.<sup class="superscript"><a href="#fn49">49</a></sup><a name="ref49"></a> The repository was formed as a result of the LDC’s collaborations with researchers, who are instrumental in evaluating the voice data collection. LDC also has agreements with 40 organisations to create a general corpus. One of them is Microsoft Research India, which deals exclusively with Indian language tagsets.<sup class="superscript"><a href="#fn50">50</a></sup><a name="ref50"></a> A ‘tag’ refers to the “labels used to indicate the part of speech”, which also include the grammatical aspects of the language.<sup class="superscript"><a href="#fn51">51</a></sup><a name="ref51"></a> A ‘tagset’ is a collection of tags made by organisations such as Microsoft that deal with corpus creation.</p>
<p> VoxForge is an open speech dataset that was set up to collect transcribed speech with Free and Open Source Speech Recognition Engines (on Linux, Windows, and Mac). <sup class="superscript"><a href="#fn52">52</a></sup><a name="ref52"></a> The submitted audio files have been made available under a General Public License (GPL) license and then compiled into acoustic models for use with Open Source speech recognition engines such as CMU Sphinx, ISIP, Julius (Github), and HTK.</p>
<p>M-AILABS Speech Dataset is the first large dataset that is available free-of-charge and usable as training data for speech recognition and speech synthesis.<sup class="superscript"><a href="#fn53">53</a></sup><a name="ref53"></a> Most of the data is derived from LibriVox (which provides free public domain audiobooks) and Project Gutenberg (which provides free e-books). The training data consists of nearly a thousand hours of audio and text files in prepared formats </p>
<p> </p>
<p> </p>
   <li><p><b> Initiatives for Indian languages </b> Speech recognition in VIs requires ‘robustness’.<sup class="superscript"><a href="#fn4">4</a></sup><a name="ref4"></a> A robust VI is characterised by having standardised models to recognise speech efficiently.<sup class="superscript"><a href="#fn5">5</a></sup><a name="ref5"></a></li></p>
<p>The National Platform for Language Technology (NPLT) is a platform for colleges, researchers, and companies to provide access to Indian language data, tools, and related web services.<sup class="superscript"><a href="#fn54">54</a></sup><a name="ref54"></a> The NPLT acts as a marketplace of linguistic resources, tools and services developed by the government, start-ups, industries, and other stakeholders. The platform makes these resources available to interested entities, be it researchers, academicians, start-ups, or MNCs, for research and commercial purposes. It acts as a marketplace for Indian language data in both speech and text, with the aim of lending power to machine learning algorithms and improving the accuracy of models. NPLT also aims to provide a central point of “discoverability of Indian Language Data, technologies and services etc" to satisfy the data needs of both industry and academia.
<p>Indic TTS is a joint initiative by the Government of India and 13 eminent Indian institutions. However, unlike Common Voice, which is a database for several languages across the world, Indic TTS focuses on 13 Indian languages. The special corpus consists of over 10,000 sentences and words spoken by both male and female speakers. The Indic TTS project also successfully launched an Android application for the TTS synthesis of 13 Indian languages. By utilising the unified parser, this application could recognise text input in 13 different Indian languages and render spoken output.</p>
<p> </p>
</ul>
</p>
<h3 id="future">Future of multilingual VIs</h3>
<p> Lawrence hints that the Hindi language will be the next most represented language in speech technology. This is attributed to the fact that India is considered an emerging market economy. Similarly, large stakeholders in the digital economy, such as Google are working on the prediction that a large percentage of next-generation Indian internet users will be Hindi speakers as opposed to English speakers. Hence, Google is now taking measures to enhance its software user interfaces and products to cater to Hindi-speaking consumers. This step is incentivised by profits, but the silver lining is that it significantly addresses the ‘digital speech divide’ dilemma.</p>
<h3 id="conclusion">Conclusion</h3>
<p> Voice-based technologies have the potential to make the internet more accessible compared to purely text-based interfaces. What people can do with the internet  can be significantly increased and improved if they can communicate in their own language. However, the need for data and the ever-changing nature of languages and their contexts can be a challenge for interfaces in multiple languages. One can hope that the push towards more voice-based interfaces and the need for language data will bring in interest and funding towards the creation of language data corpora in more languages.</p>

      </div>
      <div class="one wide column empty">
      </div>
      <div class="five wide column meta">
        <p><span id="grey">Research and Writing by</span> <br />Deepika Nandagudi Srinivasa and Shweta Mohandas
          <br />
        <span id="grey">Review and Editing by</span> <br /> Saumyaa Naidu, Puthiya Purayil Sneha, <br /><span id="grey">and</span> Pranav M.B <br />
        <span id="grey">Research Inputs by</span> <br />Sumandro Chattapadhyay<br />
	       <br />
        <a href="docs/MozVoice_PolicyBrief_02.pdf"><i class="fas fa-arrow-circle-down" style="color: black;" ></i> Download Voice Interfaces and Language
</a></p>
        <br />
         <hr />
	       <br />
        <p><span style="line-height: 3em;">CONTENTS</span></p>
	      <p><a href="#about"><strong>Background</strong></a></p>
        <p><a href="#significant"><strong>Significant challenges for multilingual support</strong></a></p>
        <p><a href="#voice"><strong>Voice initiatives to bridge the digital divide</strong></a></p>
        <p><a href="#future"><strong>Future of multilingual VIs </strong></a></p>
        <p><a href="#conclusion"><strong>Conclusion</strong></a></p>

</div>

<div class="ten wide column content">
</div>
<div class="ten wide column content">
  <br />
<h3>Notes</h3>
<table class="footnote">
  <tr>

    <td class="number">1</td>

    <td class="reference"><a name="fn1"></a>“Voice Interfaces”, <em> Infosys </em>, 2019, accessed 3 November 2021<ahref="https://www.infosys.com/services/incubating-emerging-technologies/offerings/Documents/voice-interfaces.pdf."target="_blank">https://www.infosys.com/services/incubating-emerging-technologies/offerings/Documents/voice-interfaces.pdf.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref1">&uarr;</a></span></td>
        	</tr>

        	<tr>
        	<td class="number">2</td>

        	<td class="reference"><a name="fn2"></a>Rudnicky, A. I., “The Design of Voice-driven Interfaces”, In <em>Proceedings of the Workshop on Speech and Natural Language, </em>(Association for Computational Linguistics, USA, 1989), 120–124.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref2">&uarr;</a></span></td>
        	</tr>

      	<tr>
        	<td class="number">3</td>
  	<td class="reference"><a name="fn3"></a>Rudnicky, A. I., <em> “The Design of Voice-driven Interfaces”, </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref3">&uarr;</a></span></td>

        	</tr>

  	<tr>
  <td class="number">4</td>
  <td class="reference"><a name="fn4"></a>Cole, R., et al., “The Challenge of Spoken Language Systems: Research Directions for the Nineties”, <em>IEEE Transactions on Speech and Audio Processing, </em> 3, no. 1 (1995): 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref4">&uarr;</a></span></td>
  	</tr>

               <tr>
  <td class="number">5</td>
    <td class="reference"><a name="fn5"></a>Ayesha Pervaiz, et al., “Incorporating Noise Robustness in Speech Command Recognition by Noise Augmentation of Training Data”, <em> Sensors 20 </em>, no. 8 (2020): 2336–2337,<ahref="https://doi.org/10.3390/s20082326."target="_blank">https://doi.org/10.3390/s20082326.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref5">&uarr;</a></span></td>

  	</tr>
        	<tr>
  <td class="number">6</td>
  <td class="reference"><a name="fn6"></a>Cole, R., et al., “The Challenge of Spoken Language Systems: Research Directions for the Nineties”, <em>IEEE Transactions on Speech and Audio Processing, </em> 3, no. 1 (1995): 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref6">&uarr;</a></span></td>
  	</tr>
        	<tr>
        	<td class="number">7</td>
        	<td class="reference"><a name="fn7"></a>Cole, R., et al., “The Challenge of Spoken Language Systems: Research Directions for the Nineties”, <em>IEEE Transactions on Speech and Audio Processing, </em> 3, no. 1 (1995): 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref7">&uarr;</a></span></td>

  	</tr>
        	<tr>
        	<td class="number">8</td>
        		<td class="reference"><a name="fn8"></a>Rudnicky, A. I., <em> “The Design of Voice-driven Interfaces”, </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref8">&uarr;</a></span></td>

        	</tr>
               <tr>
    	<td class="number">9</td>
  <td class="reference"><a name="fn9"></a>Dyches, H., Alemagno, S., Llorens, S. A., and Butts, J. M., “Automated Telephone-Administered Substance Abuse Screening for Adults in Primary Care”, <em>Health Care Management Science, </em> 2, no. 4 (1999): 199–204 <a href="doi:10.1023/a:1019000231214." target="_blank">doi:10.1023/a:1019000231214.  </a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref9">&uarr;</a></span></td>

      	</tr>
          	<tr>
  	<td class="number">10</td>
  <td class="reference"><a name="fn10"></a>“Usage Statistics of Content Languages for Websites”, <em> W3Techs, </em> accessed 3 November 2021 <a href="https://w3techs.com/technologies/overview/content_language."target="_blank">https://w3techs.com/technologies/overview/content_language.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref10">&uarr;</a></span></td>
  	</tr>
                <tr>
        	<td class="number">11</td>
        	<td class="reference"><a name="fn11"></a>Noack, R., “The Future of Language”, <em> Washington Post, </em> September 25,2015,<a href="https://www.washingtonpost.com/news/worldviews/wp/2015/09/24/the-future-of-language/."target="_blank">https://www.washingtonpost.com/news/worldviews/wp/2015/09/24/the-future-of-language/."</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref11">&uarr;</a></span></td>

        	</tr>
        	<tr>
        	<td class="number">12</td>

     <td class="reference"><a name="fn12"></a>The terms ‘native language’ and ‘native speaker’ are used here in the specific context of the report cited. As socio-cultural constructs, the terms have been a source of debate, particularly in postcolonial contexts and in the field of linguistics, and more recently in efforts related to language revitalisation. For more on this see: Davies, Alan. The Native Speaker: Myth and Reality. Multilingual Matters, 2003 and  O’Rourke, Bernadette. “New Speakers of Minority Languages.” <em>The Routledge Handbook of Language Revitalization,</em> 2018, 265–73.<a href=" https://doi.org/10.4324/9781315561271-33."target="_blank">https://doi.org/10.4324/9781315561271-33.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref12">&uarr;</a></span></td>


        	</tr>
        	<tr>
        	<td class="number">13</td>
  	<td class="reference"><a name="fn13"></a>Noack, R., “The Future of Language ”, <em> Washington Post. </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref13">&uarr;</a></span></td>
        	</tr>
              <tr>
  <td class="number">14</td>
  <td class="reference"><a name="fn14"></a>Noack, R., “Usage Statistics of Content Languages for Websites”, <em> W3Techs, </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref14">&uarr;</a></span></td>


  	</tr>
    <tr>
  <td class="number">15</td>
  <td class="reference"><a name="fn15"></a>Cole,“The Challenge of Spoken Language Systems”, 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref15">&uarr;</a></span></td>
        </tr>
              <tr>
  <td class="number">16</td>
        	<td class="reference"><a name="fn16"></a>Cole,“The Challenge of Spoken Language Systems”, 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref16">&uarr;</a></span></td>

        	</tr>
  <tr>
        	<td class="number">17</td>
<td class="reference"><a name="fn17"></a>Freitas, J., et al., “Spoken Language Interface for Mobile Devices”,  in Human Language Technology. Challenges of the Information Society, </em> eds. Zygmunt Vetulani, Hans Uszkoreit (Springer, Berlin, Heidelberg, 2009), 25–35. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref17">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">18</td>
  <td class="reference"><a name="fn18"></a>“RecognizedPhrase.Confidence Property”, Microsoft, accessed 17 November 2021,<a href=" https://docs.microsoft.com/en-us/dotnet/api/system.speech.recognition.recognizedphrase.confidence?view=netframework-4.8.target="_blank”>https://docs.microsoft.com/en-us/dotnet/api/system.speech.recognition.recognizedphrase.confidence?view=netframework-4.8.</a>&nbsp;&nbsp;<span class="internal-nav"><a href="#ref18">&uarr;</a></span></td>
        	</tr>
  <tr>
            	<td class="number">19</td>
              	<td class="reference"><a name="fn19"></a>Cole, “The Challenge of Spoken Language Systems”, 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref19">&uarr;</a></span></td>
        	</tr>
        	<tr>
        	<td class="number">20</td>
  	<td class="reference"><a name="fn20"></a>Cole, “The Challenge of Spoken Language Systems”, 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref20">&uarr;</a></span></td>
        	</tr>

      	<tr>
        	<td class="number">21</td>
  <td class="reference"><a name="fn21"></a>The Information Technology Act, 2000.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref21">&uarr;</a></span></td>

        	</tr>
  	<tr>
  <td class="number">22</td>
  <td class="reference"><a name="fn22"></a>Hernandez, Daniela, “How Voice Recognition Systems Discriminate Against People with Accents: When Will There be Speech Recognition for the Rest of Us?”, <em> Splinter </em> , 21 August 2015,<a href="https://splinternews.com/how-voice-recognition-systems-discriminate-against-peop-1793850122."target="_blank">https://splinternews.com/how-voice-recognition-systems-discriminate-against-peop-1793850122.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref22">&uarr;</a></span></td>
  	</tr>
               <tr>
  <td class="number">23</td>
  <td class="reference"><a name="fn23"></a>Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref23">&uarr;</a></span></td>
  	</tr>
        	<tr>
  <td class="number">24</td>
  <td class="reference"><a name="fn24"></a>Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref24">&uarr;</a></span></td>
  	</tr>
        	<tr>
  <td class="number">25</td>
        	<td class="reference"><a name="fn25"></a>Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref25">&uarr;</a></span></td>
        	</tr>
        	<tr>
        	<td class="number">26</td>
        	<td class="reference"><a name="fn26"></a> Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref26">&uarr;</a></span></td>

        	</tr>
               <tr>
    	<td class="number">27</td>
  <td class="reference"><a name="fn27"></a> Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref27">&uarr;</a></span></td>
      	</tr>
          	<tr>
  	<td class="number">28</td>
  <td class="reference"><a name="fn28"></a> Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref28">&uarr;</a></span></td>
  	</tr>
                <tr>
        	<td class="number">29</td>
        	<td class="reference"><a name="fn29"></a> Hernandez, “How Voice Recognition Systems” <em> Splinter </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref29">&uarr;</a></span></td>
     	</tr>
        	<tr>
        	<td class="number">30</td>
  <td class="reference"><a name="fn30"></a>Walkley, A. and Nagpal, J. “Why Hindi Matters in the Digital Age”,  <em> Think with Google, </em> 2015, from<ahref=”https://www.thinkwithgoogle.com/intl/en-apac/trends-and-insights/hindi-matters-digital-age/.”target="_blank”>https://www.thinkwithgoogle.com/intl/en-apac/trends-and-insights/hindi-matters-digital-age/.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref31">&uarr;</a></span></td>
        	      	<tr>
        	<td class="number">31</td>
  <td class="reference"><a name="fn31"></a>Sanchez-Stockhammer, Christina, “Hybridization in Language”, In <em> Conceptualizing Cultural Hybridization: A Transdisciplinary Approach, </em> ed. Philipp Wolfgang Stockhammer, (Springer-Verlag Berlin Heidelberg, 2012), 133-157.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref31">&uarr;</a></span></td>
        	</tr>
              <tr>
  <td class="number">32</td>
  <td class="reference"><a name="fn32"></a>Baker, S., “Will We all be Speaking Hinglish One Day?”, <em> British Council, </em> 2015, accessed 3 November 2021 <ahref=”https://www.britishcouncil.org/voices-magazine/will-we-all-be-speaking-hinglish-one-day”target="_blank”>https://www.britishcouncil.org/voices-magazine/will-we-all-be-speaking-hinglish-one-day</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref32">&uarr;</a></span></td>

  	</tr>
              <tr>
  <td class="number">33</td>

  	<td class="reference"><a name="fn33"></a>Lawrence, “Beyond the Graphic User Interface”.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref33">&uarr;</a></span></td>

        </tr>
              <tr>
  <td class="number">34</td>
        	<td class="reference"><a name="fn34"></a>Skiba, R., “Code switching as a Countenance of Language Interference”, <em> The Internet TESL Journal, </em> 3, no. 10 (1997): 1–6.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref33">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">35</td>
  <td class="reference"><a name="fn35"></a>Crystal, D. The Cambridge <em> Encyclopedia of Language, </em> (Cambridge University Press, 1987), 372-375.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref35">&uarr;</a></span></td>
         	</tr>
  <tr>
        	<td class="number">36</td>
  <td class="reference"><a name="fn36"></a>“The Challenge of Spoken Language”, 1–21.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref36">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">37</td>
  <td class="reference"><a name="fn37"></a>Martin, R., “Common Voice Languages and Accent Strategy v5”, <em> Mozilla, </em> 2020, accessed 3 November 2021,<ahref=”https://discourse.mozilla.org/t/common-voice-languages-and-accent-strategy-v5/56555”target="_blank”>,https://discourse.mozilla.org/t/common-voice-languages-and-accent-strategy-v5/56555</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref37">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">38</td>
  <td class="reference"><a name="fn38"></a>McEvoy, J., “A Few Differences Between French Spoken in Québec and France”, <em> British Council, </em> 2017, accessed 3 November 2021<ahref=”https://www.britishcouncil.org/voices-magazine/few-differences-between-french-spoken-quebec-and-france target="_blank”>https://www.britishcouncil.org/voices-magazine/few-differences-between-french-spoken-quebec-and-france</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref38">&uarr;</a></span></td>
        	</tr>
  </tr>
  <tr>
        	<td class="number">39</td>
  <td class="reference"><a name="fn39"></a>“Why Common Voice?”, <em> Common Voice, </em> <ahref=”https://commonvoice.mozilla.org/en/about”target="_blank”>https://commonvoice.mozilla.org/en/about</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref39">&uarr;</a></span></td>

        	</tr>
  <tr>
        	<td class="number">40</td>
  <td class="reference"><a name="fn40"></a>“Why Common Voice?”, <em> Common Voice. </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref40">&uarr;</a></span></td>

        	</tr>
  <tr>
        	<td class="number">41</td>
  <td class="reference"><a name="fn41"></a>Martin, R., “Common Voice Languages and Accent Strategy v5”, <em> Mozilla </em>. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref41">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">42</td>
  <td class="reference"><a name="fn42"></a>Paul, S. “Voice Is the Next Big Platform, Unless You Have an Accent”, <em> Wired, </em> 2017,<ahref=”https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/”target="_blank”>https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref42">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">43</td>
  <td class="reference"><a name="fn43"></a>Paul, S. “Voice Is the Next Big Platform, Unless You Have an Accent”, <em> Wired, </em>.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref43">&uarr;</a></span></td>

        	</tr>


  <tr>
        	<td class="number">44</td>
  <td class="reference"><a name="fn44"></a>Ali,“Why Common Voice?”, <em> Common Voice. </em> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref44">&uarr;</a></span></td>
        	</tr>
  <tr>
        	<td class="number">45</td>
  <td class="reference"><a name="fn45"></a>Ali,“Why Common Voice?”, <em> Common Voice. </em> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref45">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">46</td>
  <td class="reference"><a name="fn46"></a>Branson, M., “Help Create Common Voice’s First Target Segment”, <em> Mozilla, </em> 2020, accessed 3 November 2020<ahref=”https://discourse.mozilla.org/t/help-create-common-voices-first-target-segment/59587”target="_blank”>.https://discourse.mozilla.org/t/help-create-common-voices-first-target-segment/59587</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref46">&uarr;</a></span></td>
  	</tr>
  	</tr>
  <tr>
        	<td class="number">47</td>
  <td class="reference"><a name="fn47"></a>Branson, M., “More Data, More Languages, and Introducing our First Target Segment!”, <em> Mozilla </em>, 2020, accessed 3 November 2021 <ahref=”https://discourse.mozilla.org/t/common-voice-dataset-release-mid-year-2020/62938”target="_blank”>https://discourse.mozilla.org/t/common-voice-dataset-release-mid-year-2020/62938</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref47">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">48</td>
  <td class="reference"><a name="fn48"></a>“Mission”, <em> Linguistic Data Consortium, </em>accessed 3 November 2021,<ahref=”https://www.ldc.upenn.edu/about/mission”target="_blank”> https://www.ldc.upenn.edu/about/mission</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref48">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">49</td>
  <td class="reference"><a name="fn49"></a>“About LDC”, <em> Linguistic Data Consortium, </em>accessed 3 November 2021,<ahref=”https://www.ldc.upenn.edu/about/”target="_blank”> https://www.ldc.upenn.edu/about/</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref49">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">50</td>
  <td class="reference"><a name="fn50"></a>“Other Collaborations” <em> Linguistic Data Consortium, </em>accessed 3 November 2021,<ahref=”https://www.ldc.upenn.edu//collaborations/other”target="_blank”> https://www.ldc.upenn.edu/collaborations/other</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref50">&uarr;</a></span></td>
  	</tr>

  <tr>
        	<td class="number">51</td>
  <td class="reference"><a name="fn51"></a>“Tagset for Indian Languages”, <em> Sketch Engine, </em> accessed 3November2021<ahref=”https://www.sketchengine.eu/tagset-indian-languages/”target="_blank”>https://www.sketchengine.eu/tagset-indian-languages/</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref51">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">52</td>
  <td class="reference"><a name="fn52"></a>“VoxForge”, <em> VoxForge, </em><ahref=”http://www.voxforge.org/.”target="_blank”>http://www.voxforge.org/.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref52">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">53</td>
  <td class="reference"><a name="fn53"></a>“The M-AILABS Speech Dataset”, <em> Caito,</em> accessed 3 November 2021<ahref=”https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/.“target="_blank”>https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref53">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">54</td>
  <td class="reference"><a name="fn54"></a>“About Us”, <em> National Platform for Language Technology </em> <ahref=”https://nplt.in/demo/about-nplt,”target="_blank”>https://nplt.in/demo/about-nplt,</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref54">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">55</td>
  <td class="reference"><a name="fn55"></a>“Voices”, <em> Indic TTS </em> <ahref=” https://www.iitm.ac.in/donlab/tts/voices.php“target="_blank”>https://www.iitm.ac.in/donlab/tts/voices.php</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref55">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">56</td>
  <td class="reference"><a name="fn56"></a>“Android Applications”, <em> Indic TTS </em> <ahref=”https://www.iitm.ac.in/donlab/tts/androidapp.php”target="_blank”>https://www.iitm.ac.in/donlab/tts/androidapp.php </a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref56">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">57</td>
  <td class="reference"><a name="fn57"></a>”Lawrence, “Beyond the Graphic User Interface”.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref57">&uarr;</a></span></td>
  	</tr>
  <tr>
        	<td class="number">58</td>
  <td class="reference"><a name="fn58"></a>Hernandez, “How Voice Recognition Systems” <em> Splinter </em>&nbsp;&nbsp;<span class="internal-nav"><a href="#ref58">&uarr;</a></span></td>



</tr>
</table>
    </div>
  </div>

      <div class="six wide column empty">
      </div>


    </div>
  </div>
  <!-- Footer -->
  <div class="footer">
    <div class="ui container four column stackable grid">
      <div class="one wide column empty">
      </div>
      <div class="five wide column">
        <h3>About the Study</h3>
        <p>We believe that voice interfaces have the potential to democratise the use of the internet by addressing limitations related to reading and writing on digital text-only platforms and devices. This report examines the current landscape of voice interfaces in India, with a focus on concerns related to privacy and data protection, linguistic barriers, and accessibility for persons with disabilities (PwDs). This project was undertaken with support by the Mozilla Corporation.</p>
      </div>
      <div class="five wide column">
        <h3>Research Team</h3>
        <p>Research: Shweta Mohandas, Saumyaa Naidu, Deepika NS, Divya Pinheiro, Sweta Bisht </p>
        <p><em>Conceptualisation, Planning, and Research Inputs</em> Sumandro Chattapadhyay, Puthiya Purayil Sneha</p>
        <p><em>Illustration</em> Kruthika NS</p>
        <p><em>Website Design</em> Saumyaa Naidu</p>
        <p><em>Website Development</em> Sumandro Chattapadhyay, Pranav M Bidare</p>
        <p><em>Review and Editing</em> Puthiya Purayil Sneha, Divyank Katira, Pranav M Bidare, Torsha Sarkar, Pallavi Bedi, Divya Pinheiro</p>
        <p><em>Copy Editing</em> The Clean Copy</p>
      </div>
      <div class="four wide column">
        <h3>Copyright and Credits</h3>
        <p>Copyright: <a href="http://cis-india.org/" target="_blank">CIS, India</a>, 2021<br />License: <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC BY 4.0 International</a></p>
        <p>Built using <a href="https://semantic-ui.com/" target="_blank">Semantic UI</a><br/><a href="https://fonts.google.com/specimen/Barlow" target="_blank">Barlow</a> and <a href="https://fonts.google.com/specimen/Open+Sans" target="_blank">Open Sans</a> by <a href="https://fonts.google.com/" target="_blank">Google Fonts</a><br/>Social media icons by <a href="https://fontawesome.com/" target="_blank">Font Awesome</a><br/>Hosted on <a href="https://github.com/cis-india/mozvoice" target="_blank">GitHub</a></p>
      </div>
      <div class="one wide column empty">
      </div>
      <div class="sixteen wide column">
        <div style="float: center; clear: both;">
        <a href="https://cis-india.org/" target="_blank" style="border-bottom: 0px solid"><img src="img/logo.png" alt="The Centre for Internet and Society, India" class="logo" /></a>
        </div>
        <div class="icons" style="float: center; clear: both;">
          <a href="https://www.instagram.com/cis.india/" target="_blank"><i class="fab fa-instagram fa-lg"></i></a> <a href="https://twitter.com/cis_india" target="_blank"><i class="fab fa-twitter fa-lg"></i></a> <a href="https://www.youtube.com/channel/UC0SLNXQo9XQGUE7Enujr9Ng" target="_blank"><i class="fab fa-youtube fa-lg"></i></a></p>
	</div>
      </div>
    </div>
  </div>
</body>
</html>
