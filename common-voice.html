<!DOCTYPE html>
<html>
<head>
  <!-- Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
  <meta name="description" content="Making Voices Heard | A study by the Centre for Internet and Society, India, supported by Mozilla Corporation" />
  <!-- Title + CSS + Favicon -->
  <title>Making Voices Heard</title>
  <link rel="stylesheet" type="text/css" href="css/semantic.min.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="shortcut icon" type="image/x-icon" href="img/favicon.ico" />
  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/4c415b9185.js" crossorigin="anonymous"></script>
</head>
<body>
  <!-- Header -->
  <div>
    <div class="ui fluid container banner">
      <div class="banner-image"></div>
    </div>
  </div>
  <!-- Top Navigation Bar -->
  <div class="blue nav">
    <div class="ui container">
      <div class="nav-entries">
        <a href="index.html" id="home">Home</a> <a href="design-brief.html">Design Brief</a> <span id="inactive">Policy Brief</span> <a href="mapping-actors.html">Mapping Actors</a> <a href="#case-studies">Case Studies</a> <a href="#literature-surveys">Literature Surveys</a> <a href="#resources">Resources</a> <span id="report"><a href="docs/CIS_MakingVoicesHeard_Report.pdf"><i class="fas fa-arrow-circle-down"></i> Get Full Report</a></span>
      </div>
    </div>
  </div>
  <!-- Title -->
  <div class="grey">
    <div class="ui container four column stackable grid">
      <div class="one wide column empty">
      </div>
      <div class="fourteen wide column text">
        <h2>Common Voice</h2>
      </div>
      <div class="one wide column empty">
      </div>
      <div class="one wide column empty">
      </div>
      <div class="nine wide column text">

        <h3 id="about">About</h3>

        <p><strong> ‘... to make voice data freely and publicly available, and make sure the data represents the diversity of real people.’<sup class="superscript"><a href="#fn1">1</a></sup><a name="ref1"></a></strong></p>

          <p> Common Voice (CV) is an open-source dataset of voice recordings in multiple languages that can be used to train speech-enabled applications. With over 13, 905 validated hours recorded in 76 different languages as of July 2021<sup class="superscript"><a href="#fn2">2</a></sup><a name="ref2"></a> CV strives to create and maintain the largest publicly available voice dataset of its kind. CV believes that the availability of large public voice datasets will help foster innovation and create a healthy market for machine-learning-based speech technologies. In May 2020, CV began data collection for a single-word target segment (the recording of single words in multiple languages) or voice data for single-word sentences (for example yes and no), to be deployed for specific use cases or purposes. The exercise has begun with the digits zero through nine, as well as the words yes, no, hey and Firefox”.<sup class="superscript"><a href="#fn3">3</a></sup><a name="ref3"></a> </p>
    <h3 id="methodology">Methodology and process</h3>

        <p> CV follows a community-driven model of creating an open-source, multilingual dataset of voice recordings that is openly accessible and usable. At the same time, it has also been working on and navigating various aspects related to privacy of voice data and accessibility for persons with disabilities, which also include complex design challenges and decisions.  Some key features of this initiative include:</p>
        <h4>Community-driven contribution</h4>
        <p> “... Providing more and better data to everyone in the world who seeks to build and use voice technology.”<sup class="superscript"><a href="#fn4">4</a></sup><a name="ref4"></a> Although CV began with creating a voice dataset for English, as most of the team working on it was English-speaking, as of July 2021 there are over 76 languages on the platform. CV depends on a community of volunteers and individual users who contribute voice data in order to add new languages to its website and system. One way CV promotes localisation is by localising its website to the languages it wants to add. Before adding a new language, the community has to localise 85% of the website, so that when volunteers from the local language community visit the website, they can easily navigate it, and do not need to rely on English. Then, when the language is active on the site, it is up to the community to submit 5,000 sentences that have been recorded in that language. This indicates two things to CV: a) that there is an active language community that can provide voice recordings, and b) that the barrier to including the language in CV is fairly low. The recorded material is based on a sentence corpus that CV provides; everybody on the platform is presented with sentences that they can record and submit. These include content such as parliamentary transcripts, Wikipedia articles, and sentences that members of the community have submitted. Two other community members then check to see if the audio matches the sentences. Though this is not a foolproof system, CV reports that it has a rather high accuracy rate. If people record things that are not on the card, they get voted down very quickly. This system of community curation and regulation, therefore, adds a layer of control to the accuracy and quality of content. “Amazon and Apple, by necessity, choose languages based on what makes sense in the market and makes the most profit.”<sup class="superscript"><a href="#fn5">5</a></sup><a name="ref5"></a> Key players in the voice-as-product market serve more widely spoken languages, such as English, French, and German, because they have a large user base and hence greater demand. The issue occurs with underrepresented languages, uncommon accents, or the voices of people from underrepresented/marginalised groups – such as those belonging to particular ethnic or gender identities. As a result, large populations remain unrepresented in datasets used to train commercial voice technologies and products. This is the gap that CV is striving to diminish. CV’s data collection differs from that of start-ups and companies like Google and Amazon; here, the sentences are self-recorded by people, and CV does not automatically detect the individual’s identity, location, or other data. It does not infer the contributor’s demographic based on their browsing data. Community members are also instructed not to identify people who are in the dataset.
 </p>
      <h4> Design process and development</h4>
        <p>Since it was envisioned as a community-driven experience, the CV team applied experience design practices when conceptualising this database.<sup class="superscript"><a href="#fn6">6</a></sup><a name="ref6"></a> Like in many design problems, the project began with the identification of a need. This need was for large quantities of publicly available voice data that could be used to train speech-to-text engines. In the design process that followed, the team ideated on creating an open-source voice dataset over the course of several design thinking exercises with Mozilla community members.<sup class="superscript"><a href="#fn7">7</a></sup><a name="ref7"></a>
This resulted in paper prototypes of varying design concepts. CV then gathered in-person feedback on these prototypes to identify which design concepts to proceed on. The initial assumption of the project team was that people would need an ulterior motive to provide voice data towards this project. However, the team’s insight from the research was that most people were open to the idea of voice donation. They also inferred that people wanted to learn more about the need for such voice data collection. Hence, they designed a platform whose prominent feature was collecting voice data.<sup  class="superscript"><a href="#fn8">8</a></sup><a name="ref8"></a> They developed an interactive model where people could ‘teach’ a robot to understand human speech by reading sentences to it.<sup class="superscript"><a href="#fn9">9</a></sup><a name="ref9"></a> This robot has become part of the CV website as a mascot of sorts, even though the interactive teaching model is no longer operational. The alpha version of the CV platform was built “to tell the story of voice data and how it relates to the need for diversity and inclusivity in speech technology”.<sup class="superscript"><a href="#fn10">10</a></sup><a name="ref10"></a>
The CV team collected community feedback through tools such as Discourse<sup class="superscript"><a href="#fn11">11</a></sup><a name="ref11"></a>
and Github.<sup class="superscript"><a href="#fn12">12</a></sup><a name="ref12"></a> They developed further iterations after feedback collection and discourse analysis. The Open Innovation team at Mozilla shared with us that they emphasise prototyping and reiterating. They carried out a user experience (UX) audit of the working prototype and considered community feedback from Github and Discourse. Based on this assessment, they made refinements to the platform.

Following the release of the working version, the CV team conducted another UX audit. They took into account a combination of UX heuristics, competitor evaluation (such as of platforms such as Headspace)<sup class="superscript"><a href="#fn13">13</a></sup><a name="ref13"></a>, and community feedback. They looked at community feedback on Github and Discourse and spoke to the engineers who built CV. Since 2017, the focus has been on improving the platform and primarily enhancing the experience of contributing voice data. Presently, the team is looking at the bigger picture by focusing on fine-tuning the contributors’ experience based on the data and research accumulated.</p>
          <h3 id="enabling">Enabling multi-language contributions/h3>
        <p>The main motivation for the project was to address the unavailability of voice data in Indian languages. The functioning of a TTS system is dependent on the training data that is fed into the system, which includes speech .wav files along with a transcript of the corresponding text. The TTS project aims to develop text-to-speech synthesisers for 13 Indian languages, which could help researchers and developers work on Indian voice applications. One of the goals of the project is to make the voice of the text-to-speech system sound as natural and understandable as possible. The first phase of the project concentrated on three languages (3 Indo-Aryan languages and 3 Dravidian languages), the second phase added 7 more languages to the study.</p>
        <h3 id="access">Access and accessibility</h3>
        <p>The TTS project was started with the idea of giving people with disabilities access to regional information on the internet, such as news reports in Indian languages. Since the consortium is a publicly funded project, the datasets and research have been made public on its website. The datasets are available free of cost to researchers – they just need to log in to the website to use them. Start-ups and businesses that want to use the data can sign a Memorandum Of Understanding with Indic TTS and access the data.</p>

      <h3 id="privacy">Privacy and data collection</h3>
        <p>As stated earlier, the text data for training the systems was taken from publicly available sources such as online news portals, Wikipedia pages, websites, and blogs; hence, privacy and data protection are not significant concerns. Additionally, with regard to the speech data, the readings were done by professional voice artists who recorded sounds and words for the project based on a script provided to them by the researchers.</p>

     <h3 id="challenges">Challenges</h3>

        <p>One of the main challenges for the researchers was ensuring that the datasets were comprehensive and accurate while keeping the cost of creating and accessing them low. Since the project was publicly funded, the researchers needed to work with the available funding and ensure that the research was accessible and free. As stated earlier, the data and the research are open to researchers,  and start-ups can request the data after signing an MOU. Another challenge was making the speech output sound more human-like and less robotic, similar to the heavily funded and data-rich interfaces of Amazon and Google. The other challenge was making the output speech systems context-specific, such as with children’s books.
        Voice interfaces provide accessibility support for individuals who are unable to see the screen or understand the text. However, no applications other than Google and Amazon claim to provide accessibility features. Amazon Echo’s website lists the various features that customers with vision, hearing, mobility, and speech accessibility needs could use. <sup class="superscript"><a href="#fn18">18</a></sup><a name="ref18"></a> Google Home provides accessibility features that allow the individual to control appliances and entertainment, make phone calls, broadcast messages, and manage tasks in addition to its voice assistant.<sup class="superscript"><a href="#fn19">19</a></sup><a name="ref19"></a></p>

      <h3 id="future">Future of Indic TTS</h3>
        <p>The project looks at continuing research and data collection with the help of government funding. Given the scale and amount of funding needed for such projects, including the requirement of infrastructure and trained human resources, the government is the primary source of funding. With the new funding from the Ministry of Electronics and Information Technology, the researchers at IITM have started a project to make English lecture videos available in Indian languages. The objective of this project is to make lectures in different domains, like humanities, healthcare, etc., freely accessible to students in their languages. This is a small-scale project, and Indic TTS hopes to expand it to more languages and subjects. </p>
        <br />
    <i> Disclaimer: This is an independent case study conducted as a part of the Making Voices Heard Project, supported by the Mozilla Corporation. The researchers have not received any external remuneration as a part of this case study, and claim no conflict of interest.</i>
      </div>
      <div class="one wide column empty">
      </div>
      <div class="five wide column meta">
        <p><span id="grey">Research and Writing by</span> <br />Shweta Mohandas and Saumyaa Naidu
          <br />
        <span id="grey">Review and Editing by</span> <br />Puthiya Purayil Sneha, <br /><span id="grey">and</span> Torsha Sarkar<br />
        <span id="grey">Research Inputs by</span> <br />Sumandro Chattapadhyay<br />
	       <br />
        <a href="docs/MozVoice_PolicyBrief_02.pdf"><i class="fas fa-arrow-circle-down" style="color: black;" ></i> Download Policy Brief</a></p>
        <br />
         <hr />
	       <br />
        <p><span style="line-height: 3em;">CONTENTS</span></p>
	      <p><a href="#about"><strong>About</strong></a></p>
        <p><a href="#methodology"><strong>Methodology and Process</strong></a></p>
        <p><a href="#languages"><strong>Languages</strong></a></p>
        <p><a href="#access"><strong>Access and accessibility</strong></a></p>
        <p><a href="#privacy"><strong>Privacy and data collection </strong></a></p>
        <p><a href="#challenges"><strong>Challenges</strong></a></p>
        <p><a href="#future"><strong>Future of Indic TTS</strong></a></p>
</div>

<div class="ten wide column content">
</div>
<div class="ten wide column content">
  <br />
<h3>Notes</h3>
<table class="footnote">
  <tr>
    <td class="number">1</td>
<td class="reference"><a name="fn1"></a> Baby, Arun et al., "Resources for Indian Languages", In <em>Proceedings of CBBLR workshop, International Conference on Text, Speech and Dialogue</em>. Springer, 2016.&nbsp;&nbsp;<span class="internal-nav"><a href="#ref1">&uarr;</a></span></td>

      	</tr>

<tr>
        	<td class="number">2</td>
      	<td class="reference"><a name="fn2"></a>“Indic TTS”, Indic TTS;Department of Electronics and Information Technology (DEITY) has been renamed to Ministry of Electronics and Information Technology (MEITY) 03 November 2021, <a href="https://www.iitm.ac.in/donlab/tts/ " target="_blank"> https://www.iitm.ac.in/donlab/tts/</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref2">&uarr;</a></span></td>
      	</tr>


<tr>
        	<td class="number">3</td>
    <td class="reference"><a name="fn3"></a> “Indic TTS”,<em> Indic TTS.</em> Accessed 3 November 2021. &nbsp;&nbsp;<span class="internal-nav"><a href="#ref3">&uarr;</a></span></td

      	</tr>


<tr>
        	<td class="number">4</td>
    <td class="reference"><a name="fn4"></a>Baby Arun, "Resources for Indian Languages"&nbsp;&nbsp;<span class="internal-nav"><a href="#ref4">&uarr;</a></span></td

      	</tr>
<tr>
        	<td class="number">5</td>

<td class="reference"><a name="fn5"></a> “An Introduction to Optical Character Recognition for Beginners”, Towards Data Science, accessed 5 January 2022, <a href="https://towardsdatascience.com/an-introduction-to-optical-character-recognition-for-beginners-14268c99d60"target="_blank">https://towardsdatascience.com/an-introduction-to-optical-character-recognition-for-beginners-14268c99d60 </a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref5">&uarr;</a></span></td>

      	</tr>

<tr>
        	<td class="number">6</td>

<td class="reference"><a name="fn6"></a> Tan,Zhixing et al., “Neural machine translation: A review of methods, resources, and tools”, AI Open Volume 1.(2020):5-21, <a href="https://doi.org/10.1016/j.aiopen.2020.11.001."target="_blank">https://doi.org/10.1016/j.aiopen.2020.11.001. </a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref6">&uarr;</a></span></td>

      	</tr>

<tr>
        	<td class="number">7</td>
    <td class="reference"><a name="fn7"></a> “Indic TTS”,<em> Indic TTS.</em>&nbsp;&nbsp;<span class="internal-nav"><a href="#ref7">&uarr;</a></span></td

      	</tr>

<tr>
        	<td class="number">8</td>

<td class="reference"><a name="fn8"></a>  Baby, Arun, “A Unified Approach to Speech Synthesis in Indian Languages”, (MS Thesis, IIT Madras, 2019), 1–93,<a href=" https://www.arunbaby.com/assets/docs/MSthesis_2019.pdf."target="_blank"> https://www.arunbaby.com/assets/docs/MSthesis_2019.pdf.</a> &nbsp;&nbsp;<span class="internal-nav"><a href="#ref8">&uarr;</a></span></td>


</tr>
</table>
    </div>
  </div>

      <div class="six wide column empty">
      </div>


    </div>
  </div>
  <!-- Footer -->
  <div class="footer">
    <div class="ui container four column stackable grid">
      <div class="one wide column empty">
      </div>
      <div class="five wide column">
        <h3>About the Study</h3>
        <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Elementum facilisis leovel fringilla est ullamcorper. Faucibus scelerisque eleifend donec pretium. Nunc vel risus commodo viverra. In hendrerit gravida rutrum quisque non.
Egestas sed sed risus pretium. Nulla porttitor massa id neque aliquam.</p>
      </div>
      <div class="five wide column">
        <h3>Research Team</h3>
        <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Elementum facilisis leovel fringilla est ullamcorper. Faucibus scelerisque eleifend donec pretium. Nunc vel risus commodo viverra. In hendrerit gravida rutrum quisque non. Egestas sed sed risus pretium. Nulla porttitor massa id neque aliquam.</p>
      </div>
      <div class="four wide column">
        <h3>Copyright and Credits</h3>
        <p>Copyright: <a href="http://cis-india.org/" target="_blank">CIS, India</a>, 2021<br />License: <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC BY 4.0 International</a></p>
        <p>Built using <a href="https://semantic-ui.com/" target="_blank">Semantic UI</a><br/><a href="https://fonts.google.com/specimen/Barlow" target="_blank">Barlow</a> and <a href="https://fonts.google.com/specimen/Open+Sans" target="_blank">Open Sans</a> by <a href="https://fonts.google.com/" target="_blank">Google Fonts</a><br/>Social media icons by <a href="https://fontawesome.com/" target="_blank">Font Awesome</a><br/>Hosted on <a href="https://github.com/cis-india/mozvoice" target="_blank">GitHub</a></p>
      </div>
      <div class="one wide column empty">
      </div>
      <div class="sixteen wide column">
        <div style="float: center; clear: both;">
        <a href="https://cis-india.org/" target="_blank" style="border-bottom: 0px solid"><img src="img/logo.png" alt="The Centre for Internet and Society, India" class="logo" /></a>
        </div>
        <div class="icons" style="float: center; clear: both;">
          <a href="https://www.instagram.com/cis.india/" target="_blank"><i class="fab fa-instagram fa-lg"></i></a> <a href="https://twitter.com/cis_india" target="_blank"><i class="fab fa-twitter fa-lg"></i></a> <a href="https://www.youtube.com/channel/UC0SLNXQo9XQGUE7Enujr9Ng" target="_blank"><i class="fab fa-youtube fa-lg"></i></a></p>
	</div>
      </div>
    </div>
  </div>
</body>
</html>
